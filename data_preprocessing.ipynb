{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_csv(file_path, columns):\n",
    "    return pd.read_csv(file_path) if os.path.exists(file_path) else pd.DataFrame(columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"unique_sites_count.csv\"\n",
    "output_file_trusted = \"../data/trusted_sites.csv\"\n",
    "trusted_owners = {\"ТСН\", \"Суспільне | Новини\", \"Лачен пише\", \"Bihus.Info\", \"BBC News Україна\", \"BBC NEWS Україна\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [os.path.join(\"..\", \"data\", f\"news_part_{i}.csv\") for i in range(1, 24)]\n",
    "site_counts = load_or_create_csv(output_file, [\"site\", \"owner\", \"count\"])\n",
    "trusted_sites= load_or_create_csv(output_file_trusted, [\"url\", \"owner\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed D:/Лаби/3 курс/2 семестр/МЛ/Проект/data/news_part_1.csv\n",
      "Processed D:/Лаби/3 курс/2 семестр/МЛ/Проект/data/news_part_2.csv\n",
      "Processed D:/Лаби/3 курс/2 семестр/МЛ/Проект/data/news_part_3.csv\n",
      "Processed D:/Лаби/3 курс/2 семестр/МЛ/Проект/data/news_part_4.csv\n",
      "Processed D:/Лаби/3 курс/2 семестр/МЛ/Проект/data/news_part_5.csv\n",
      "Processed D:/Лаби/3 курс/2 семестр/МЛ/Проект/data/news_part_6.csv\n",
      "Processed D:/Лаби/3 курс/2 семестр/МЛ/Проект/data/news_part_7.csv\n",
      "Processed D:/Лаби/3 курс/2 семестр/МЛ/Проект/data/news_part_8.csv\n",
      "Processed D:/Лаби/3 курс/2 семестр/МЛ/Проект/data/news_part_9.csv\n",
      "Processed D:/Лаби/3 курс/2 семестр/МЛ/Проект/data/news_part_10.csv\n",
      "Processed D:/Лаби/3 курс/2 семестр/МЛ/Проект/data/news_part_11.csv\n",
      "Processed D:/Лаби/3 курс/2 семестр/МЛ/Проект/data/news_part_12.csv\n",
      "Processed D:/Лаби/3 курс/2 семестр/МЛ/Проект/data/news_part_13.csv\n",
      "Processed D:/Лаби/3 курс/2 семестр/МЛ/Проект/data/news_part_14.csv\n",
      "Processed D:/Лаби/3 курс/2 семестр/МЛ/Проект/data/news_part_15.csv\n",
      "Processed D:/Лаби/3 курс/2 семестр/МЛ/Проект/data/news_part_16.csv\n",
      "Processed D:/Лаби/3 курс/2 семестр/МЛ/Проект/data/news_part_17.csv\n",
      "Processed D:/Лаби/3 курс/2 семестр/МЛ/Проект/data/news_part_18.csv\n",
      "Processed D:/Лаби/3 курс/2 семестр/МЛ/Проект/data/news_part_19.csv\n",
      "Processed D:/Лаби/3 курс/2 семестр/МЛ/Проект/data/news_part_20.csv\n",
      "Processed D:/Лаби/3 курс/2 семестр/МЛ/Проект/data/news_part_21.csv\n",
      "Processed D:/Лаби/3 курс/2 семестр/МЛ/Проект/data/news_part_22.csv\n",
      "Processed D:/Лаби/3 курс/2 семестр/МЛ/Проект/data/news_part_23.csv\n",
      "Data preprocessing completed\n"
     ]
    }
   ],
   "source": [
    "for i, file in enumerate(file_list):\n",
    "    df = pd.read_csv(file)\n",
    "    df = df.dropna()\n",
    "    df[\"site\"] = df[\"url\"].apply(lambda x: urlparse(x).netloc)\n",
    "\n",
    "    df_counts = df[[\"site\", \"owner\"]].value_counts().reset_index()\n",
    "    df_counts.columns = [\"site\", \"owner\", \"count\"]\n",
    "    \n",
    "    site_counts = pd.concat([site_counts, df_counts])\n",
    "    site_counts = site_counts.groupby([\"site\", \"owner\"], as_index=False).sum()\n",
    "    \n",
    "    site_counts = site_counts.dropna().sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "    site_counts.to_csv(output_file, index=False)\n",
    "    print(f\"Processed {file}\")\n",
    "    \n",
    "    del df, df_counts\n",
    "    gc.collect()\n",
    "    \n",
    "print(\"Data preprocessing completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect news from trusted sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ..\\data\\news_part_1.csv\n",
      "Processed ..\\data\\news_part_2.csv\n",
      "Processed ..\\data\\news_part_3.csv\n",
      "Processed ..\\data\\news_part_4.csv\n",
      "Processed ..\\data\\news_part_5.csv\n",
      "Processed ..\\data\\news_part_6.csv\n",
      "Processed ..\\data\\news_part_7.csv\n",
      "Processed ..\\data\\news_part_8.csv\n",
      "Processed ..\\data\\news_part_9.csv\n",
      "Processed ..\\data\\news_part_10.csv\n",
      "Processed ..\\data\\news_part_11.csv\n",
      "Processed ..\\data\\news_part_12.csv\n",
      "Processed ..\\data\\news_part_13.csv\n",
      "Processed ..\\data\\news_part_14.csv\n",
      "Processed ..\\data\\news_part_15.csv\n",
      "Processed ..\\data\\news_part_16.csv\n",
      "Processed ..\\data\\news_part_17.csv\n",
      "Processed ..\\data\\news_part_18.csv\n",
      "Processed ..\\data\\news_part_19.csv\n",
      "Processed ..\\data\\news_part_20.csv\n",
      "Processed ..\\data\\news_part_21.csv\n",
      "Processed ..\\data\\news_part_22.csv\n",
      "Processed ..\\data\\news_part_23.csv\n",
      "Фільтрація завершена, всі файли оброблено.\n"
     ]
    }
   ],
   "source": [
    "for i, file in enumerate(file_list):\n",
    "        df = pd.read_csv(file)\n",
    "        df = df.dropna()\n",
    "\n",
    "        trusted_sites = df[df[\"owner\"].isin(trusted_owners)][[\"url\", \"owner\", \"text\"]]\n",
    "        trusted_sites.to_csv(output_file_trusted, sep=\";\", encoding=\"utf-8\", index=False, mode=\"a\", header=False)\n",
    "        \n",
    "        print(f\"Processed {file}\")\n",
    "        gc.collect()\n",
    "        del df, trusted_sites\n",
    "\n",
    "print(\"Фільтрація завершена, всі файли оброблено.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split trusted news file into smaller (each 98mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 1\n",
      "Saved chunk 2\n",
      "Saved chunk 3\n",
      "Saved chunk 4\n",
      "Saved chunk 5\n",
      "Saved chunk 6\n",
      "Saved chunk 7\n",
      "Saved chunk 8\n",
      "Saved chunk 9\n",
      "Data split completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "output_dir = \"data/splited_trusted_news\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(output_file_trusted, sep=\";\").sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "chunk_size = 99 * 1024 * 1024  \n",
    "rows_per_chunk = int(chunk_size / (df.memory_usage(deep=True).sum() / len(df)))\n",
    "\n",
    "for i, start in enumerate(range(0, len(df), rows_per_chunk)):\n",
    "    df.iloc[start:start + rows_per_chunk].to_csv(f\"{output_dir}/trusted_news_part_{i+1}.csv\", sep=\";\", index=False)\n",
    "    print(f\"Saved chunk {i+1}\")\n",
    "\n",
    "print(\"Data split completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
